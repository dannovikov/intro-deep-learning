{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c60954-0c1e-4394-b543-ba387faf07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here I was playing around with making my own custom model wtih skip connections between each convolution layer. \n",
    "I had to make sure the convolutions didn't affect the image size. Unfortunatly I haven't had much success. \n",
    "\n",
    "I call it \"SkipNet\" - feel free to use it if you want a 30% accurate bird classifier!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0ce243-69c6-46b1-996c-d40d8de24717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from datetime import datetime\n",
    "#from ignite.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22885eeb-3bac-4e7f-afe3-3bf481b4bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7860b1-953a-420c-b4a9-cc588d795b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(output, labels):\n",
    "    _, preds = torch.max(output, dim=1)\n",
    "\n",
    "    return torch.sum(preds == labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d23bf4-cfd5-4b89-8b7c-b097ba24ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Data formatting:\n",
    "pretrained models expect mini-batches of 3-channel RGB images of shape (N, 3, H, W), where\n",
    "N is the number of images, H and W are expected to be at least 224\n",
    "pixels. The images have to be loaded in to a range of [0, 1] and then\n",
    "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
    "\n",
    "https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/\n",
    "'''\n",
    "data_dir = './birds_400'\n",
    "trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = ImageFolder(f'{data_dir}/train', transform=trans)\n",
    "test_ds  = ImageFolder(f'{data_dir}/test', transform=trans)\n",
    "valid_ds = ImageFolder(f'{data_dir}/valid', transform=trans)\n",
    "\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, BATCH_SIZE, shuffle=False, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0197b91e-5612-48d3-81bd-676bbe68db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61630c2-9741-48df-9421-017782889d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=3*112*112, out_features=4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = x\n",
    "        for conv in [self.c1, self.c2, self.c3]:\n",
    "            out = self.relu(conv(out)) + identity\n",
    "            identity = out\n",
    "        out = self.maxpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        logits = self.fc2(out)\n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001abc05-cc53-48e1-8b57-d1eac45f706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipNet(num_classes=400)\n",
    "#model = torch.load('skipnet.model')\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.1,\n",
    "    patience=0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1caf7327-0dba-4aa5-a5b8-8fd835255ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df97bb5-cb01-4b68-99ac-946a4f9ee3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training at  2022-03-27 14:37:07.277133\n",
      "Train loss: 0.007110989387450127 Val loss: 5.896547245979309 Val Accuracy: 0.3175\n",
      "Train loss: 0.007104849686833379 Val loss: 5.896923136711121 Val Accuracy: 0.3175\n",
      "Train loss: 0.0070992718480227595 Val loss: 5.897259092330932 Val Accuracy: 0.3175\n",
      "Train loss: 0.007092243109108906 Val loss: 5.897614192962647 Val Accuracy: 0.3175\n",
      "epoch 5: 308/584\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30940/2443111632.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch {epoch+1}: {batch_idx}/{len(train_dl)}\\r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "print(\"Beginning training at \", datetime.now())\n",
    "history = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_dl):\n",
    "        print(f\"epoch {epoch+1}: {batch_idx}/{len(train_dl)}\\r\", end=\"\", flush=True)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    history.append(train_loss / (batch_idx + 1))\n",
    "    print(f\"Train loss: {history[-1]}\", end=\" \", flush=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = 0, 0\n",
    "        for batch_idx, (images, labels) in enumerate(valid_dl):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            pred = model(images)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            acc = Accuracy(pred, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "        val_loss /= batch_idx + 1\n",
    "        val_acc /= batch_idx + 1\n",
    "    scheduler.step(val_acc / (batch_idx + 1))\n",
    "    print(f\"Val loss: {val_loss} Val Accuracy: {val_acc}\", flush=True)\n",
    "\n",
    "print(f\"Validation loss: {val_loss}\\nValidation Accuracy: {val_acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5ab45-a4de-4fd0-8454-8c932368e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I only ran this a few times to show the loss was already at a local minimum, and that the accuracy plateaud at 0.3175.\n",
    "I feel this is the limit of this model.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27a2b5-a58d-4ea5-8b3b-e951c1d535bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./skipnet.model\")\n",
    "print(\"Model has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2eb6b9-d7d6-415c-8503-a89212afe8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b2b01-3072-46fe-a555-fc0b90c3898c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f292b4-6a48-4d63-95db-a22d56c738ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55f77c-1d4e-4cb7-939b-2b0679f1b488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8beac8-6530-4b03-b73d-c7044e204e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c55c95-f6f2-41c7-a157-a0f1c1030bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
