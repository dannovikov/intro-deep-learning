{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Daniel Novikov\n",
    "01/24/2022\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34b86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Fill a 7x7 ndarray with random values from 0-6\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# Create random matrix whos values are in half-open interval [0,1)\n",
    "A = np.random.random(size=(7,7))\n",
    "\n",
    "# Scale matrix to [0,6)\n",
    "A = A * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f98011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.66279879, 3.4467793 , 3.02615031, 5.24405768, 5.75042026,\n",
       "        2.38288046, 4.41486161],\n",
       "       [3.75701251, 2.79088303, 0.57411969, 2.40618275, 5.01024484,\n",
       "        5.40991078, 3.11044261],\n",
       "       [4.82898829, 1.84837744, 1.62750103, 5.14826048, 1.60106611,\n",
       "        1.09430518, 1.53090274],\n",
       "       [4.42029881, 2.21162304, 2.40180053, 5.893175  , 1.7428154 ,\n",
       "        4.8308312 , 1.60493761],\n",
       "       [2.29731443, 2.67837472, 2.54920944, 1.83536508, 3.30366842,\n",
       "        0.52034668, 1.5122764 ],\n",
       "       [1.68711733, 3.17387378, 3.37045707, 2.16985308, 4.96656714,\n",
       "        5.82903309, 5.48269412],\n",
       "       [5.45209529, 1.13629273, 3.14860238, 2.57685948, 3.07941636,\n",
       "        3.01329224, 5.10984569]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31e1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8666b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 6 3 4 2 3 4 1 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 5., 6.],\n",
       "       [5., 6., 3.],\n",
       "       [6., 3., 4.],\n",
       "       [3., 4., 2.],\n",
       "       [4., 2., 3.],\n",
       "       [2., 3., 4.],\n",
       "       [3., 4., 1.],\n",
       "       [4., 1., 6.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Write a program that takes takes a n-vector and creates a matrix,\n",
    "whos rows consist of a sliding window of 3 adjacent components in the vector.\n",
    "The window starts at the beginning of the vector, slides over one cell at a time,\n",
    "and stops when the last variable of the window reaches the end of the vector.\n",
    "\n",
    "'''\n",
    "\n",
    "n = 10\n",
    "A = np.random.random(n) * 7\n",
    "A = A.astype(int)\n",
    "\n",
    "def solve(A):\n",
    "    if len(A) < 3: return A\n",
    "    result = np.ndarray(shape=(0,3))\n",
    "    for x in range(len(A)-2):\n",
    "        result = np.vstack([result, (A[x], A[x+1], A[x+2])])\n",
    "    return result\n",
    "\n",
    "print(A)\n",
    "solve(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3. How can we save time using stochastic gradient compared to standard gradient descent? Do we have a better method? '''\n",
    "\n",
    "'''\n",
    "Standard gradient descent (GD) calculates error on the entire dataset after each learning step.\n",
    "Stochastic gradient descent (SGD) calculates error on each incoming datapoint one at a time, taking a learning step each time.\n",
    "\n",
    "Thus SGD takes learning steps more often than GD. And after each learning step, the choice of parameters get less wrong.\n",
    "\n",
    "We expect to converge onto local minima faster with SGD as we take many learning steps quickly.\n",
    "\n",
    "It is more likely that SGD will find a solution after a single pass through the data;\n",
    "unlike GD, which makes an entire pass through the data with each learning step.\n",
    "\n",
    "A better method seems to be batch gradient desccent, which takes learning steps on subsets of data. \n",
    "This combines the speed of SGD with the learning step quality that multiple  datapoints on each update step.\n",
    "\n",
    "Also, adding a momemntum term can help us converge faster and be more resilient to noise.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f451d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
